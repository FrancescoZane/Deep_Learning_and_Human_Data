{"cells":[{"cell_type":"markdown","metadata":{"id":"rXuOgrQCzOOQ"},"source":["# Machine learning tools for dimensionality reduction: PCA and clustering\n","\n","When using battery-constrained sensors, one wants to reduce the amount of information to be transmitted - to save energy resources - while guaranteeing the correct recovery of the data at the receiver end. This is achieved through the implementation of dimensionality reduction tools that extract and retain the most relevant information from the data. In this notebook, you will implement and evaluate different approaches for dimensionality reduction using PCA and clustering. You will became familiar with two clustering algorithms (i.e., K-means and DBSCAN) and you will implement the clustering following two different approaches (i.e., cluster the raw signals or some relevant features extracted from the signals).\n","\n","In summary, in this notebook, you will:\n","1. implement the principal component analysis (PCA) algorithm from scratch\n","2. use the PCA, K-means and DBSCAN functions provided by Python developers through the scikit-learn library\n","3. apply the algorithms to ECG signals\n","4. evlaluate the efficacy of the dimensionality reduction through PCA and clustering"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"e0HtAWEYzk_3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Select the correct working directory with `cd`. This directory should be your own copy (not the shared directory)!"],"metadata":{"id":"oVfrlfwQ3p8p"}},{"cell_type":"code","source":["cd 'drive/MyDrive/HDA_labs/Lab_1/'"],"metadata":{"id":"E1P_KT7F3bOM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CJm-NIz1zOOT"},"source":["## Import statements\n","IMPORTANT: `ModuleNotFoundError` usually means you miss some packets. Install them with `pip` as shown below."]},{"cell_type":"code","source":["!pip install kneebow\n","!pip install py-ecg-detectors\n","!pip install EntropyHub"],"metadata":{"id":"DVmsNXb8zO39"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Uii9jknzOOT"},"outputs":[],"source":["# Scientific packages\n","import numpy as np\n","import scipy\n","import matplotlib\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import mean_squared_error\n","from sklearn.model_selection import train_test_split\n","from sklearn.cluster import DBSCAN\n","from kneebow.rotor import Rotor\n","from scipy.io import loadmat\n","from scipy.signal import medfilt\n","\n","from scipy.signal import resample\n","\n","# Plot packages\n","import matplotlib.pyplot as plt\n","from matplotlib.gridspec import GridSpec\n","\n","# Custom functions\n","from hda_utils import load_dataset, load_dataset_DBSCAN, segment_ECG, segment_ECG2, matrix_to_signal\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = [15,10]\n","np.random.seed(1)"]},{"cell_type":"markdown","metadata":{"id":"Sa5HnLrZzOOV"},"source":["## Load and visualize the ECG signal we will use"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TqeDa8hVzOOV"},"outputs":[],"source":["# Load signal\n","ecg_signal = load_dataset()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ndqslSPmzOOV"},"outputs":[],"source":["# Plot\n","fig = plt.figure(constrained_layout=True)\n","gs = GridSpec(1, 1, figure=fig)\n","ax1 = fig.add_subplot(gs[0, :])\n","\n","ax1.plot(ecg_signal)\n","ax1.set_title('Raw ECG signal (whole recording)', fontsize=14)\n","ax1.set_xlabel('Samples', fontsize=12)\n","ax1.set_ylabel('ECG(mV)', fontsize=12)\n","ax1.grid(axis='y')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Ss-HaLcgzOOW"},"source":["## ECG segmentation\n","\n","At this point, we need to implement a way to segment the signal into beats.\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1tZtPoQbDHjAD0U3rzJx8k9Ogc8HGz7YR\" style=\"width:800px;\">\n","<caption><center> <u> Figure 1 </u>: one heart beat extracted from the ECG signal</center></caption>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X-UxP_HtzOOW"},"outputs":[],"source":["# ECG segmentation\n","fs = 130\n","ecg_mat, r_peaks, original_len = segment_ECG(ecg_signal, fs)\n","\n","\n","# Plot segments\n","fig1 = plt.figure(constrained_layout=True)\n","gs = GridSpec(2, 1, figure=fig1)\n","ax1 = fig1.add_subplot(gs[0, :])\n","ax2 = fig1.add_subplot(gs[1, :])\n","\n","ax1.plot(ecg_signal, 'r', zorder = 0 )\n","ax1.scatter(r_peaks, list(ecg_signal[r_peaks].T[0]), zorder = 1)\n","ax1.set_title('Segmented original ECG', fontsize=14)\n","ax1.set_xlabel('Samples', fontsize=12)\n","ax1.set_ylabel('ECG(uV)', fontsize=12)\n","ax1.grid(axis='y')\n","\n","ax2.plot(ecg_mat.T)\n","ax2.set_title('ECG segments', fontsize=14)\n","ax2.set_xlabel('Samples', fontsize=12)\n","ax2.set_ylabel('ECG(uV)', fontsize=12)\n","ax2.grid(axis='y')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"rBtLWJ6yzOOX"},"source":["Next, an important step of data preprocessing for machine learning algorithms is normalization.\n","Here we use the [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) class in Python that provides functions to normalize the data. In the following cell, normalize the ECG data by using the `StandardScaler().fit(ecg_mat)` and `scaler.transform(ecg_mat)`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sDq9l9f-zOOX"},"outputs":[],"source":["# Normalize data along feature axis (2 lines of code)\n","scaler = None\n","ecg_mat_norm = None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yHOBjxOZzOOY"},"outputs":[],"source":["# Split data\n","train_fraction = 0.8\n","\n","# Vector to select only test data from original signal\n","x_axis = np.arange(r_peaks[int(np.floor(len(r_peaks)*train_fraction))-1],r_peaks[-1], 1)\n","\n","ecg_mat_train, ecg_mat_test, original_len_train, original_len_test = train_test_split(ecg_mat_norm, original_len, train_size=train_fraction, shuffle = False)"]},{"cell_type":"markdown","metadata":{"id":"At8j4sIDzOOY"},"source":["## 1 - Principal component analysis (PCA)\n","\n","The goal of PCA is to find a new basis to project the data in the dataset $X \\in \\mathbb{R}^{n \\times m}$. The new basis is obtained as a linear combination of the vectors in the original basis, and the dataset after applying PCA is obtained as $Y=XP$ with $P \\in \\mathbb{R}^{m \\times m}$ and $Y \\in \\mathbb{R}^{n \\times m}$.\n","\n","After performing PCA, we can retain a lower number of dimensions $d < m$ to reduce the dimensionality of the input space, obtaining $\\hat{Y}=XW$ with $W \\in \\mathbb{R}^{m \\times d}$.\n","\n","The objective is to minimize the mean square error between $\\hat{X} = \\hat{Y}\\hat{W}$ and $X$, where $\\hat{W} \\in \\mathbb{R}^{d \\times m}$ is the matrix of reverse transformation:\n","\n","$$\\|\\hat{X} - X\\|^2_2$$\n","\n","\n","In geometric terms, we want to find $d$ axes along which most of the variance occurs."]},{"cell_type":"markdown","metadata":{"id":"UDi0HFlBzOOY"},"source":["### 1a - implement your own dimensionality reduction algorithm based on PCA\n","The steps are the following:\n","1. normalize the data (in our case, already done above by StandardScaler)\n","2. compute the covariance matrix using the [`cov` function](https://numpy.org/doc/stable/reference/generated/numpy.cov.html) of the numpy library: take a look at the `rowvar` argument to properly compute the covariance (in our case the rows of the matrix are the observations and the columns represent the variables/features)\n","3. perform eigen decomposition using the [`linalg.eigh` function](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.eigh.html#scipy-linalg-eigh) of the scipy library: the function returns the eigenvectors and eigenvalues sorted based on the value of the eigenvalues\n","5. select the `num_components` strongest eigenvectors to be retained\n","\n","The steps until here allow creating the model for dimensionality reduction through PCA. The training data is used to make the model learn which are the components of the data and their relevance in the definition of the data.\n","\n","Once the model has been trained through steps 2-5, it can be used in real-time to process new data and obtain a compression version of it. The compressed data are built by retaining the strongest components found through PCA. As stated before, this compression step can be helpful when data needs to be transmitted over a communication system and we want to reduce the amount of information to send for different reasons such as latency, battery constraints, or network congestion issues. Therefore, the last step is to\n","6. apply the transformation to the test data and go back to the original reference system, see the mathematical formulation above\n","\n","Complete the following function by substituting the 'None' with the correct statements."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fvR0qzrtzOOZ"},"outputs":[],"source":["def my_PCA(X_train, X_test, num_components, scaler, original_len = None):\n","\n","    covariance_mat = None\n","\n","    eigenvalues, eigenvectors = None\n","    sorted_indeces = np.flip(np.argsort(eigenvalues))\n","    P = eigenvectors[:, sorted_indeces]\n","    print(P.shape)\n","    W = None\n","    Y_hat = None\n","    W_hat = None\n","    X_hat = None\n","\n","    approx_data = scaler.inverse_transform(X_hat)\n","    ecg_sig_rec = matrix_to_signal(approx_data, original_len)\n","\n","    return ecg_sig_rec"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SZhS_BeTzOOZ"},"outputs":[],"source":["# No compression\n","n_components_nocomp = ecg_mat_train.shape[0]\n","ecg_sig_nocomp = my_PCA(ecg_mat_train, ecg_mat_test, n_components_nocomp, scaler, original_len_test)\n","\n","# Compressed signal\n","n_components_comp1 = 3\n","ecg_sig_comp1 = my_PCA(ecg_mat_train,ecg_mat_test, n_components_comp1, scaler, original_len_test)\n","\n","# More compressed signal\n","n_components_comp2 = 1\n","ecg_sig_comp2 = my_PCA(ecg_mat_train, ecg_mat_test, n_components_comp2, scaler, original_len_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jc_eas9QzOOZ"},"outputs":[],"source":["fig2 = plt.figure(constrained_layout=True)\n","gs = GridSpec(3, 1, figure=fig2)\n","ax1 = fig2.add_subplot(gs[0, :])\n","ax2 = fig2.add_subplot(gs[1, :])\n","ax3 = fig2.add_subplot(gs[2, :])\n","\n","# No compression\n","ax1.plot(x_axis, ecg_sig_nocomp, zorder = 1)\n","ax1.plot(ecg_signal, zorder = 0)\n","ax1.set_title('NO COMPRESSION: 100% of variance (' + str(n_components_nocomp) + ' components)', fontsize = 14)\n","ax1.set_xlabel('Samples', fontsize=12)\n","ax1.set_ylabel('ECG(uV)', fontsize=12)\n","ax1.grid(axis='y')\n","\n","# Compressed signal\n","ax2.plot(x_axis, ecg_sig_comp1, zorder = 1)\n","ax2.plot(ecg_signal, zorder = 0)\n","ax2.set_title('COMPRESSION: ' + str(n_components_comp1) + ' components', fontsize = 14)\n","ax2.set_xlabel('Samples', fontsize=12)\n","ax2.set_ylabel('ECG(uV)', fontsize=12)\n","ax2.grid(axis='y')\n","\n","# More compressed signal\n","ax3.plot(x_axis, ecg_sig_comp2, zorder = 1)\n","ax3.plot(ecg_signal, zorder = 0)\n","ax3.set_title('COMPRESSION: ' + str(n_components_comp2) + ' components', fontsize = 14)\n","ax3.set_xlabel('Samples', fontsize=12)\n","ax3.set_ylabel('ECG(uV)', fontsize=12)\n","ax3.grid(axis='y')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"T8dSotD8zOOa"},"source":["### 1b - use the PCA dimensionality reduction algorithm from Python developers\n","Use the [sklearn.decomposition.PCA class](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) from the scikit-learn library. The steps are the following:\n","1. import the library (already done, see the \"Import statements\" section above)\n","2. create a new object of the class with `PCA(...)`: using this class you can specify the number of components to retain (as you did with `my_PCA` function) or the explainable variance (take a look at the documentation!). In this part of the notebook, you will specify the explainable variance you want to keep.\n","3. fit the PCA model on the data $X$ using the [fit method](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA.fit): this provides you with $Y$ being the new representation of $X$ using a new basis. Use: `pca.fit(X_train)`\n","\n","As already mentioned above, the steps until here are necessary to create the model and hence are performed using the training data.\n","\n","At this point we can use the built model to compress new data (test data):\n","4. compress the test data using the compressed PCA transformation with [transform method](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA.transform). Use: `pca.transform(X_test)`\n","5. go back to the original space using the [inverse_transform method](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA.inverse_transform). Use: `pca.inverse_transform(Y_hat)`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UpjjMkswzOOa"},"outputs":[],"source":["def PCA_variancefraction(X_train, X_test, scaler, original_len, variance_fraction = None):\n","    # Create a PCA model with the required explained variance\n","    pca = None\n","    # Fit the model to the train data\n","    None\n","\n","    n_components = pca.n_components_\n","\n","    # Transform the test data\n","    Y_hat = None\n","    # Go back to original space\n","    X_hat = None\n","\n","    approx_data = scaler.inverse_transform(X_hat)\n","    ecg_sig_rec = matrix_to_signal(approx_data, original_len)\n","\n","    return ecg_sig_rec, n_components"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gpIHSytFzOOa"},"outputs":[],"source":["# Use the function considering two values as example for the variance\n","variance_fractions = [.93, .9]\n","\n","# No compression\n","ecg_sig_nocomp, n_components_nocomp = PCA_variancefraction(ecg_mat_train, ecg_mat_test, scaler, original_len_test, None)\n","\n","# Compressed signal\n","ecg_sig_comp1, n_components_comp1 = PCA_variancefraction(ecg_mat_train, ecg_mat_test, scaler, original_len_test, variance_fractions[0])\n","\n","# More compressed signal\n","ecg_sig_comp2, n_components_comp2 = PCA_variancefraction(ecg_mat_train, ecg_mat_test, scaler, original_len_test, variance_fractions[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gkfhOBcyzOOa"},"outputs":[],"source":["fig2 = plt.figure(constrained_layout=True)\n","gs = GridSpec(3, 1, figure=fig2)\n","ax1 = fig2.add_subplot(gs[0, :])\n","ax2 = fig2.add_subplot(gs[1, :])\n","ax3 = fig2.add_subplot(gs[2, :])\n","\n","# No compression\n","ax1.plot(x_axis, ecg_sig_nocomp, zorder = 1)\n","ax1.plot(ecg_signal, zorder = 0)\n","ax1.set_title('NO COMPRESSION: 100% of variance (' + str(n_components_nocomp) + ' components)', fontsize = 14)\n","ax1.set_xlabel('Samples', fontsize=12)\n","ax1.set_ylabel('ECG(uV)', fontsize=12)\n","ax1.grid(axis='y')\n","\n","# Compressed signal\n","ax2.plot(x_axis,ecg_sig_comp1, zorder = 1)\n","ax2.plot(ecg_signal, zorder = 0)\n","ax2.set_title('COMPRESSION: ' + str(variance_fractions[0]*100) + '% of variance (' + str(n_components_comp1) + ' components)', fontsize = 14)\n","ax2.set_xlabel('Samples', fontsize=12)\n","ax2.set_ylabel('ECG(uV)', fontsize=12)\n","ax2.grid(axis='y')\n","\n","# More compressed signal\n","ax3.plot(x_axis, ecg_sig_comp2, zorder = 1)\n","ax3.plot(ecg_signal, zorder = 0)\n","ax3.set_title('COMPRESSION: ' + str(variance_fractions[1]*100) + '% of variance (' + str(n_components_comp2) + ' components)', fontsize = 14)\n","ax3.set_xlabel('Samples', fontsize=12)\n","ax3.set_ylabel('ECG(uV)', fontsize=12)\n","ax3.grid(axis='y')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"s69qtW3vzOOb"},"source":["## 1c - Error computation\n","\n","The root-mean-square error (RMSE) metric can be used to evaluate the differences between the values predicted by a model and the actual values (the ground truth).\n","\n","It is calculated as:\n","$$RMSE = \\|\\hat{X} - X\\|^2_2$$\n","where $\\hat{X}$ is the predicted data and $X$ represents the observed data.\n","\n","Proprieties of RMSE:\n","1. non-negative\n","2. RMSE = 0 -> no error (almost never achieved in practice)\n","3. In general, a lower RMSE is better than a higher one, there is no upper limit.\n","4. As the measure depends on the scale of the data, the comparison across different types of data requires a normalization step.\n","5. RMSE is sensitive to outliers.\n","6. It is the measure usually used in the implementation of K-means and others clustering techniques.\n","\n","To obtain the error, you can use the [mean_squared_error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) method of sklearn library."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0qDoRgGSzOOb"},"outputs":[],"source":["# The error is computed on the test dataset\n","test_ecg_signal = ecg_signal[x_axis]\n","\n","nocomp_error = None\n","comp1_error = None\n","comp2_error = None\n","\n","print('NO COMPRESSION - RMSE: \\t' + str(nocomp_error))\n","print('COMPRESSION, ' + str(variance_fractions[0]*100) + '% of variance - RMSE: \\t' + str(comp1_error))\n","print('COMPRESSION, ' + str(variance_fractions[1]*100) + '% of variance - RMSE: \\t' + str(comp2_error))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R-0d6rulzOOb"},"outputs":[],"source":["def plot_errors_comps(data_train, data_test, scaler,original_len_test, x_axis, nocomp_error, n_components_nocomp, variance_fractions):\n","    variance_fractions_plot = 1 - np.logspace(max(np.log10(2)-1, (np.log10(1-min(variance_fractions)))),np.log10(1)-3,1000, endpoint=True)\n","    n_components_plot = np.zeros(variance_fractions_plot.shape[0])\n","    errors_plot = np.zeros(variance_fractions_plot.shape[0])\n","\n","    for i in range(variance_fractions_plot.shape[0]):\n","        ecg_sig_plot, n_components_plot[i] = PCA_variancefraction(data_train, data_test, scaler, original_len_test, variance_fractions_plot[i])\n","        errors_plot[i] = mean_squared_error(test_ecg_signal, ecg_sig_plot, squared=False)\n","\n","    n_components_plot = np.hstack((n_components_plot, n_components_nocomp))\n","    errors_plot = np.hstack((errors_plot, nocomp_error))\n","    variance_fractions_plot = np.hstack((variance_fractions_plot,1))\n","\n","    return n_components_plot, errors_plot, variance_fractions_plot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c-hidwg7zOOb"},"outputs":[],"source":["n_components_plot, errors_plot, variance_fractions_plot = plot_errors_comps(ecg_mat_train, ecg_mat_test, scaler, original_len_test,x_axis, nocomp_error, n_components_nocomp, variance_fractions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XsumaB2lzOOc"},"outputs":[],"source":["fig3 = plt.figure(constrained_layout=True)\n","gs = GridSpec(2, 1, figure=fig3)\n","ax = fig3.add_subplot(gs[0, :])\n","ax_n = fig3.add_subplot(gs[1, :])\n","\n","major_ticks_x = np.arange(0, 1.05, 0.05)\n","minor_ticks_x = np.arange(0, 1.01,.01)\n","major_ticks_y = np.arange(int(min(errors_plot)), max(errors_plot), 5)\n","minor_ticks_y = np.arange(int(min(errors_plot)), max(errors_plot), 1)\n","\n","ax.set_xticks(major_ticks_x)\n","ax.set_xticks(minor_ticks_x, minor=True)\n","ax.set_yticks(major_ticks_y)\n","ax.set_yticks(minor_ticks_y, minor=True)\n","ax_n.set_xticks(major_ticks_x)\n","ax_n.set_xticks(minor_ticks_x, minor=True)\n","\n","ax.grid(which='minor', alpha=0.1)\n","ax.grid(which='major', alpha=0.5)\n","ax_n.grid(which='minor', alpha=0.1)\n","ax_n.grid(which='major', alpha=0.5)\n","\n","ax.set_xlabel('Variance fraction', fontsize=12)\n","ax.set_ylabel('RMSE', fontsize=12)\n","ax.set_title('RMSE of compressed signal', fontsize = 14)\n","ax_n.set_xlabel('Variance fraction', fontsize=12)\n","ax_n.set_ylabel('N components', fontsize=12)\n","ax_n.set_title('Number of components required', fontsize = 14)\n","\n","ax.plot(variance_fractions_plot, errors_plot, zorder = 0)\n","ax_n.plot(variance_fractions_plot, n_components_plot, zorder = 0)\n","\n","ax.scatter(variance_fractions, [comp1_error, comp2_error], color = 'r', zorder = 1, label = \"Your values\")\n","ax.scatter(1, [nocomp_error], color = 'g', zorder = 1, label = \"Full variance\")\n","ax_n.scatter(variance_fractions, [n_components_comp1, n_components_comp2], color = 'r', zorder = 1, label = \"Your values\")\n","ax_n.scatter(1, [n_components_nocomp], color = 'g', zorder = 1, label = \"Full variance\")\n","\n","ax.legend(fontsize = 12)\n","ax_n.legend(fontsize = 12)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"rN_LsYpwzOOc"},"source":["## 2 - Clustering with the KMeans Algorithm\n","\n","Compression can be obtained also by means of clustering techniques.\n","This is achieved by first obtaining relevant clusters by training a clustering algorithm on the training data.\n","Next, new data are clustered by using the clusters defined during the training phase. Having obtained the clusters, each signal is coded (substituted) with the centroid word of the associated cluster.\n","\n","In the transmission scenario, after the dictionary of centroids is created, you can transmit only the index of the word instead of the entire signal.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xJM2J0nAzOOc"},"outputs":[],"source":["# Number of clusters\n","nclusters = 5"]},{"cell_type":"markdown","metadata":{"id":"v1GuUuAhzOOc"},"source":["See the sklearn documentation about the KMeans clustering:\n","[k-means](https://scikit-learn.org/stable/modules/clustering.html#k-means)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uFAVPrtOzOOc"},"outputs":[],"source":["# Create the model\n","kmeans = None\n","\n","# Fit the model\n","None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kzfbj9-szOOc"},"outputs":[],"source":["# Train labels\n","train_labels = kmeans.labels_\n","print(\"Train labels: \" + str(train_labels))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OdUWM7wnzOOc"},"outputs":[],"source":["# Plot centroids\n","\n","centroids = kmeans.cluster_centers_\n","\n","fig4 = plt.figure(figsize=(15,4), constrained_layout=True)\n","for i in range(nclusters):\n","    plt.plot(scaler.inverse_transform(centroids[i,:].reshape(1,-1)).reshape(-1,1), label = 'Centroid ' + str(kmeans.predict(centroids)[i]))\n","plt.legend(loc = 'upper left', fontsize = 12)\n","plt.title('K-means clustering centroids, K = ' + str(nclusters), fontsize = 14)\n","plt.xlabel('Samples', fontsize=12)\n","plt.ylabel('ECG(uV)', fontsize=12)\n","plt.grid()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1pkuumMWzOOc"},"outputs":[],"source":["# Plot clusters\n","# Each clusters consists of similar ECG segments\n","\n","fig5 = plt.figure(figsize=(15,nclusters*2), constrained_layout=True)\n","gs = GridSpec(nclusters//2+nclusters%2, 2, figure=fig5)\n","\n","axs = []\n","for i in range(nclusters):\n","    axs.append(fig5.add_subplot(gs[i//2, i%2]))\n","    axs[i].plot(scaler.inverse_transform(centroids[i,:].reshape(1,-1)).reshape(-1,1))\n","    axs[i].set_xlabel('Samples', fontsize=12)\n","    axs[i].set_ylabel('ECG(uV)', fontsize=12)\n","    axs[i].grid()\n","for i in range(ecg_mat_train.shape[0]):\n","    axs[kmeans.labels_[i]].plot(scaler.inverse_transform(ecg_mat_train[i,:].reshape(1,-1)).reshape(-1,1))\n","\n","plt.suptitle('K-means clusters, K = ' + str(nclusters), fontsize = 14)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ft8z5683zOOd"},"source":["It's now time to use the trained clustering algorithm. For each segment of the test data, predict the correct cluster using the KMeans model.\n","Use the [predict](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans.predict) method as:`kmeans.predict(ecg_mat_test)`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rYShERy0zOOd"},"outputs":[],"source":["# For each segment to decode, predict the correct cluster\n","test_labels = None\n","test_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P0TX4gTUzOOd"},"outputs":[],"source":["# Then substitute the original ECG segment with its centroid\n","ecg_decoded = np.zeros(ecg_mat_test.shape)\n","centroids_dict = dict(zip(kmeans.predict(centroids), centroids))\n","\n","for i in range(ecg_mat_test.shape[0]):\n","        ecg_decoded[i,:] = scaler.inverse_transform(centroids_dict[test_labels[i]].reshape(1,-1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DhQxIYDqzOOd"},"outputs":[],"source":["# Error calculation\n","ecg_kmeans_sig = matrix_to_signal(ecg_decoded, original_len_test)\n","ecg_kmeans_error = mean_squared_error(ecg_signal[x_axis], ecg_kmeans_sig, squared=False)\n","print('RMSE on test data: ' + str(ecg_kmeans_error))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uSh3tuHkzOOd"},"outputs":[],"source":["fig6 = plt.figure(figsize=(15,4))\n","plt.plot(x_axis, ecg_kmeans_sig, zorder = 1)\n","plt.plot(ecg_signal, zorder = 0)\n","plt.title('KMeans compressed ECG, K = ' + str(nclusters), fontsize = 14)\n","plt.xlabel('Samples', fontsize=12)\n","plt.ylabel('ECG(uV)', fontsize=12)\n","plt.grid(axis='y')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"OjjwozsZzOOd"},"source":["## 3 - Clustering after PCA dimensionality reduction\n","Now, you will perform the clustering on $\\hat{Y}$, the signal in the transformed space obtained with a reduced number of PCA components."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bc2XtdtIzOOd"},"outputs":[],"source":["num_components = 15\n","pca2 = PCA(n_components = num_components)\n","pca2.fit(ecg_mat_train)\n","Y_hat_train = pca2.transform(ecg_mat_train)\n","Y_hat_test = pca2.transform(ecg_mat_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"veca5bkGzOOh"},"outputs":[],"source":["K2 = 5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rAfT6flEzOOh"},"outputs":[],"source":["kmeans2 = KMeans(n_clusters = K2, random_state=0).fit(Y_hat_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IaKMvGm9zOOh"},"outputs":[],"source":["train_labels = kmeans2.labels_\n","print(\"Train labels: \" + str(train_labels))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rFmxvSQozOOh"},"outputs":[],"source":["test_labels = kmeans2.predict(Y_hat_test)\n","print(\"Test labels: \" + str(test_labels))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e-5XPjJCzOOh"},"outputs":[],"source":["fig, ax1 = plt.subplots(ncols=1)\n","\n","from random import randint\n","colors = []\n","\n","for i in range(K2):\n","    colors.append('#%06X' % randint(0, 0xFFFFFF))\n","\n","cmap = [matplotlib.colormaps['Set1'](i) for i in range(nclusters)]\n","\n","for l in range(K2):\n","    ax1.scatter(\n","        Y_hat_train[train_labels == l, 0],\n","        Y_hat_train[train_labels == l, 1],\n","        color=cmap[l],\n","        label=\"Train data: class %s\" % l,\n","        marker = 'o'\n","    )\n","    ax1.scatter(\n","        Y_hat_test[test_labels == l, 0],\n","        Y_hat_test[test_labels == l, 1],\n","        color=cmap[l],\n","        label=\"Test data: class %s\" % l,\n","        marker = '*'\n","    )\n","\n","    ax1.legend()\n","    ax1.set_title(\"Data clusters\", fontsize = 14)\n","    ax1.set_xlabel(\"First PCA component\", fontsize = 12)\n","    ax1.set_ylabel(\"Second PCA component\", fontsize = 12)\n","    ax1.grid()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JRFDwTCczOOi"},"outputs":[],"source":["# Then substitute the original ECG segment with the centroid obtained after the clustering over PCA components\n","centroids2 = kmeans2.cluster_centers_\n","ecg_decoded2 = np.zeros(ecg_mat_test.shape)\n","ecg_centroids = pca2.inverse_transform(centroids2) # transform back the centroids to the ECG signal space\n","centroids_dict2 = dict(zip(kmeans2.predict(centroids2), ecg_centroids)) # associate to each cluster label, the correspondent centroid\n","\n","for i in range(ecg_mat_test.shape[0]):\n","    ecg_decoded2[i,:] = scaler.inverse_transform(centroids_dict2[test_labels[i]].reshape(1,-1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fjK2za8hzOOi"},"outputs":[],"source":["# Error calculation\n","ecg_kmeans_sig2 = matrix_to_signal(ecg_decoded2, original_len_test)\n","ecg_kmeans_error2 = mean_squared_error(ecg_signal[x_axis], ecg_kmeans_sig2, squared=False)\n","print('RMSE on test data: ' + str(ecg_kmeans_error2))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i5y_kx1AzOOi"},"outputs":[],"source":["fig8 = plt.figure(figsize=(15,4))\n","plt.plot(x_axis, ecg_kmeans_sig2, zorder = 1)\n","plt.plot(ecg_signal, zorder = 0)\n","plt.title('KMeans compressed ECG after PCA, K = ' + str(nclusters) + ', components = ' + str(num_components), fontsize = 14)\n","plt.xlabel('Samples', fontsize=12)\n","plt.ylabel('ECG(uV)', fontsize=12)\n","plt.grid()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"kl7tJd09zOOi"},"source":["## 4 - Clustering with DBSCAN\n","\n","With K-Means the number of clusters must be fixed in advance. However, sometimes, the number of clusters is a variable we would like to find rather than having it fixed in advance.\n","\n","DBSCAN, one of the most popular clustering algorithms, automatically finds the number of clusters based on the local density of the data points.\n","\n","It is necessary to choose two parameters in order to run DBSCAN:\n","1. `epsilon`: The radius of local neighborhoods (defined as epsilon-neighborhoods);\n","2. `MinPts`: Minimum number of points in an epsilon-neighborhood.\n","\n","We will use the Scikit-Learn implementation of DBSCAN, available here: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html"]},{"cell_type":"markdown","metadata":{"id":"o8JgW6dRzOOi"},"source":["### 4.1 - Data Preprocessing\n","\n"," - This time, we will use a different set of ECG measurements. We will load a sequence of measurements where the first half of the data belongs to a set of anomalous heart-beat events, while the remaining half will correspond to regular heart-beat events.  \n"," - After data segmentation, we will use a different pre-processing strategy: instead of using the whole signal in the feature matrix, we will actually extract a set of features characterizing the heart beat events. For details about the extracted features you can check the `feature_extract` function implementation available in the `hda_utils.py` file."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8axM5A1kzOOi"},"outputs":[],"source":["ecg_signal = load_dataset_DBSCAN()\n","\n","# ECG segmentation\n","fs = 300\n","feature_mat, segments_list, r_peaks, original_len = segment_ECG2(ecg_signal, fs)\n","\n","# Normalize data along feature axis - use StandardScaler as before (2 lines of code)\n","scaler = StandardScaler().fit(feature_mat)\n","ecg_mat_norm = scaler.transform(feature_mat)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TW6nvedAzOOj"},"outputs":[],"source":["# Plot segments\n","fig1 = plt.figure(constrained_layout=True)\n","gs = GridSpec(2, 1, figure=fig1)\n","ax1 = fig1.add_subplot(gs[0, :])\n","ax2 = fig1.add_subplot(gs[1, :])\n","\n","ax1.plot(ecg_signal, 'r', zorder = 0 )\n","ax1.scatter(r_peaks, list(ecg_signal[r_peaks].T[0]), zorder = 1)\n","ax1.set_title('Segmented original ECG', fontsize=14)\n","ax1.set_xlabel('Samples', fontsize=12)\n","ax1.set_ylabel('ECG(uV)', fontsize=12)\n","ax1.grid(axis='y')\n","\n","for segment in segments_list:\n","    ax2.plot(segment)\n","# ax2.plot(segments_list)\n","ax2.set_title('ECG segments', fontsize=14)\n","ax2.set_xlabel('Samples', fontsize=12)\n","ax2.set_ylabel('ECG(uV)', fontsize=12)\n","ax2.grid(axis='y')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ynT72yOXzOOj"},"outputs":[],"source":["# Let's apply PCA with 0.9 explained variance\n","# Let's see how many features are really meaningful for explaining the data\n","pca2 = None\n","pca2.fit(ecg_mat_norm)\n","Y_hat = pca2.transform(ecg_mat_norm)\n","print(pca2.n_components_)\n","print(pca2.explained_variance_ratio_)"]},{"cell_type":"markdown","metadata":{"id":"lmxVOOcQzOOj"},"source":["### 4.2 - Parameter optimization\n","\n","For the selection of `eps` we will use the technique seen in the theory lectures. Given a fixed value of `MinPts`:\n","- For each point in the dataset, compute the average distance from all its MinPts-Nearest Neighbors;\n","- Plot the obtained distances in ascending order;\n","- Find the knee point of the curve: the y value will be the optimal value of `eps`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8uO0LA9lzOOj"},"outputs":[],"source":["min_points = 10\n","\n","## Compute the distance matrix using the Euclidean distance  - use the np.linalg.norm function\n","npoints = Y_hat.shape[0]\n","\n","dist_matrix = np.zeros((npoints, npoints)) # empty vector that you will fill in the for loop\n","for i in range(npoints):\n","    for j in range(npoints):\n","        dist_matrix[i,j] = None\n","\n","## For each point compute the average distance from its min_points closest points\n","avg_dist = np.zeros(npoints)  # empty vector that you will fill in the for loop\n","for i, row in enumerate(dist_matrix):\n","    # Sort the distances in ascending order (you need this in the next row to select the `min_points` closest points)\n","    sorted_distances = None\n","    # Compute the average considering only the `min_points` closest points, i.e., select only the first min_points elements from sorted_distances without considering the first element (that is the point itself)\n","    avg_dist[i] = None\n","\n","## Sort avg_dist\n","sorted_avg_dist = np.sort(avg_dist)\n","dists_2d = np.stack([np.arange(1, npoints+1), sorted_avg_dist], axis=1)\n","\n","# Find the knee point\n","rotor = Rotor()\n","rotor.fit_rotate(dists_2d)\n","elbow_idx = rotor.get_elbow_index()\n","epsilon = sorted_avg_dist[elbow_idx]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IvbyRagizOOj"},"outputs":[],"source":["# plot the sorted distances and the knee point\n","fig7 = plt.figure(figsize=(8,8))\n","plt.grid(alpha=0.5, zorder=0)\n","plt.plot(sorted_avg_dist, zorder=1)\n","plt.scatter(elbow_idx, epsilon, color='r', zorder=2)\n","plt.hlines(epsilon, 0, elbow_idx, alpha=0.5, color='gray', linestyle='--', zorder=1)\n","plt.vlines(elbow_idx, 0, epsilon, alpha=0.5,color='gray', linestyle='--', zorder=1)\n","plt.title('Sorted average distances', fontsize = 14)\n","plt.xlabel('Samples', fontsize=12)\n","plt.ylabel('Distance', fontsize=12)\n","\n","print(f\"MinPts value: {min_points}; Best value for epsilon: {str(sorted_avg_dist[elbow_idx])} \")"]},{"cell_type":"markdown","metadata":{"id":"CF0oQu5EzOOj"},"source":["### 4.3 - Apply DBSCAN algorithm\n","\n","The syntax is very similar to the `PCA` function:\n","1. import the DBSCAN library (already done, see the `import` statements section at the beginning of the notebook)\n","2. create a new object of the class with `DBSCAN(...)`: using this class you can specify the parameters `eps` and  `min_samples` (take a look at the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)!). Here, you will input the optimized parameters we found in the above cells.\n","3. fit the DBSCAN model on the PCA transformed data (`Y_hat`) using the [fit method](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN.fit): this will return a fitted instance of the `DBSCAN` object.\n","4. Find the clustering labels in the `labels_` member variable of the `DBSCAN` object instance."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5EAmr46hzOOk"},"outputs":[],"source":["# Run DBSCAN algorithm with the chosen parameters\n","dbscan = None\n","# `fit` dbscan on Y_hat\n","None\n","\n","# DBSCAN assigns a cluster label for each point, and -1 if the point is classified as noise\n","print(dbscan.labels_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ftd7bxjwzOOk"},"outputs":[],"source":["# Plot clusters\n","nclusters = len(np.unique(dbscan.labels_))\n","\n","fig5 = plt.figure(figsize=(15,nclusters*2), constrained_layout=True)\n","gs = GridSpec(nclusters//2+nclusters%2, 2, figure=fig5)\n","\n","axs = []\n","for i in range(nclusters):\n","    axs.append(fig5.add_subplot(gs[i//2, i%2]))\n","\n","for i in range(Y_hat.shape[0]):\n","    axs[dbscan.labels_[i]].plot(segments_list[i])\n","    axs[dbscan.labels_[i]].set_title('Cluster ' + str(dbscan.labels_[i]), fontsize = 14)\n","\n","plt.suptitle(f\"DBSCAN Clusters, MinPts = {min_points}, Epsilon = {epsilon:.2f}\", fontsize = 14)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iD1c41d5zOOk"},"outputs":[],"source":["## Visualize the clusters in the PCA space and in the original input sequence\n","\n","nclusters = len(np.unique(dbscan.labels_))\n","labels = dbscan.labels_\n","\n","\n","fig, ax = plt.subplots(2,1, gridspec_kw={'height_ratios': [2,1]})\n","\n","cmap = [matplotlib.colormaps['Set1'](i) for i in range(nclusters)]\n","\n","\n","for l in range(nclusters):\n","    ax[0].scatter(\n","        Y_hat[dbscan.labels_+ 1 == l, 0],\n","        Y_hat[dbscan.labels_ + 1== l, 1],\n","        color=cmap[l],\n","        label=f\"Cluster {l-1}\",\n","        marker = 'o',\n","        zorder=1\n","    )\n","\n","ax[0].legend()\n","ax[0].set_title(\"Data clusters\", fontsize = 14)\n","ax[0].set_xlabel(\"First PCA component\", fontsize = 12)\n","ax[0].set_ylabel(\"Second PCA component\", fontsize = 12)\n","ax[0].set_aspect('equal', 'box')\n","ax[0].grid(0.5, zorder=0)\n","\n","current_x = 0\n","for i, segment in enumerate(segments_list):\n","    ax[1].plot(range(current_x, len(segment) + current_x), segment, color=cmap[labels[i]+1], zorder=1)\n","    current_x += len(segment)\n","\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"eUsZwN5izOOk"},"source":["# Conclusions\n","Both PCA and clustering techniques can be used for data compression.\n","In the notebook we saw that with PCA we can control the number of components or the amount of variance we want to keep.\n","We analyzed two clustering strategies. At first, we saw that with KMeans we control the number of clusters K and we have a more clear visualization of which samples are outliers. Second, we applied DBSCAN to automatically obtain the number of clusters based on the similarities among the samples, and we saw how to optimally chose the parameters needed for the method. We tried two clustering approaches: clustering the raw signals and clustering relevant time-domain and frequency-domain features extracted from the signal.\n","Clustering and PCA can also be combined for dimensionality reduction: clustering in the transformed space may reduce the overall RMSE. Depending on your application you should take into account both the compression error and the computational cost of each approach to design the most appropriate solution to your problem.\n","\n","You can now play with the notebook modifying it...enjoy!"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}